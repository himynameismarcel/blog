<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>Binary Choice Models - Some Theory and an Application - himynameismarcel</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<meta property="og:title" content="Binary Choice Models - Some Theory and an Application" />
<meta property="og:description" content="Abstract In this slightly longer post I want to outline some of the differences in the econometric modeling techniques when the dependent variable is a binary variable with two distinct outcomes (yes/no, 0/1). We will discuss the shortcomings of ordinary least squares (OLS) in these settings and how the concept of a conditional expectation function (CEF) is replaced by the concept of a conditional probability function (CPF) that gives rise to the so called logit and probit models used for binary responses." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/binary-choice-models-some-theory-and-an-application/" />
<meta property="article:published_time" content="2014-06-14T00:00:00+00:00" />
<meta property="article:modified_time" content="2014-06-14T00:00:00+00:00" />

	
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	
	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<link rel="stylesheet" href="/css/tomorrow-night-eighties.css" rel="stylesheet" id="theme-stylesheet">
	<script src="/js/highlight.pack.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
	<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<script async type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
	<div class="container">
		<div class="logo">
			<a class="logo__link" href="/" title="himynameismarcel" rel="home">
				<div class="logo__title">himynameismarcel</div>
				<div class="logo__tagline">A bit of economics, data and programming</div>
			</a>
		</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/about/">
				
				<span class="menu__text">About</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
</header>

		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Binary Choice Models - Some Theory and an Application</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class="meta__text" datetime="2014-06-14T00:00:00Z">June 14, 2014</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg><span class="meta__text"><a class="meta__link" href="/categories/econometrics/" rel="category">Econometrics</a>
	</span>
</div></div>
		</header><div class="content post__content clearfix">
			


<div id="abstract" class="section level1">
<h1>Abstract</h1>
<p>In this slightly longer post I want to outline some of the differences in the
econometric modeling techniques when the dependent variable is a binary
variable with two distinct outcomes (yes/no, 0/1). We will discuss the
shortcomings of ordinary least squares (OLS) in these settings and how
the concept of a conditional expectation function (CEF) is replaced by
the concept of a conditional probability function (CPF) that gives rise
to the so called <strong>logit</strong> and <strong>probit</strong> models used for binary responses.
Within these frameworks, the least squares principle is replaced by
maximum likelihood estimation, which is <em>the</em> estimation technique
common to all applications where the response variable is of qualitative
nature. The theoretical discussion will be complemented with an
illustrative example.</p>
</div>
<div id="introduction-and-theory" class="section level1">
<h1>Introduction and Theory</h1>
<p>Ordinary Least Squares (OLS) is undoubtedly the most frequently used
estimation technique within the Classical Linear (Regression) Model
(CLM) to recover a functional and foremost causal relationship between a
dependent variable of interest <span class="math inline">\(y\)</span> and explanatory variables
(regressors) <span class="math inline">\(x_j\)</span> with <span class="math inline">\(j = 1, ..., k\)</span> by means of parameter
estimation. But the framework of the CLM underlies certain important
assumptions for why within this framework the OLS-principle should be
used only in circumstances where there are good reasons to believe that
the underlying assumptions are fulfilled.</p>
<p>In many applications the response variable <span class="math inline">\(y\)</span> we are trying to model is
available on a continuous (or sufficiently continuous) measurement scale
and in many instances also (roughly) normally distributed, at least
after adequate transformations (e.g. log-transformations etc.). But this
changes if we want to model categorial (or qualitative) variables, which
are always discrete and can be binary/multinomial/ordered or
quantitative variables with a restricted range (e.g., counts, durations,
censored, truncated) that can be both, continuous or discrete. Within
the class of categorial variables, the models used differ with respect
to the response variable we are trying to model (hence they are called
binary, multinomial or ordered response models).<br/>
Binary variables arise from the answer to yes/no questions, multinomial
extends the binary variables to the setting where we have more than two
mutually exclusive and exhaustive categories that cannot be ordered and
finally ordered variables are like multinomial variables but with a
meaningful ordering of the categories.</p>
<p>The OLS-principle assumes relationships between <em>quantitative</em>
variables, where the regressors can enter the model also as dummy
variables and the response is assumed to be continuous. For
<span class="math inline">\(i = 1, ..., n\)</span>, the PRF (population regression function) for the simple
linear model can be written as</p>
<p><span class="math display">\[\begin{eqnarray*}
y_i &amp; = &amp; \beta_1 + \beta_2 x_{2i} + ... + \beta_k x_{ki} + \epsilon_i  \\
&amp; = &amp; \sum_{j=1}^K\beta_jx_{ji} + \epsilon_i  \\
&amp; = &amp; \mathbf{x_i&#39;\beta} + \epsilon_i.
\end{eqnarray*}\]</span></p>
<p>which can be compactly written as
<span class="math display">\[\mathbf{y} = \mathbf{X \beta} + \mathbf{\epsilon}.\]</span> Under the
Gauß-Markov assumptions which are (1) linearity in parameters, (2) full
rank of the design matrix, (3) exogeneity of the explanatory variables
<span class="math inline">\(E(\mathbf{\epsilon}\vert \mathbf{X})=0\)</span> and homoscedastic and
uncorrelated errors, meaning that the variance/covariance matrix simply
is <span class="math inline">\(Cov\mathbf{(\epsilon)} = \sigma^2 \mathbf{I}\)</span>, the ordinary least
squares estimator is the best linear unbiased estimator (BLUE) and we
can rewrite the model in terms of the then correctly specified
<em>conditional</em> expectation function</p>
<p><span class="math display">\[\begin{eqnarray*}
E(\mathbf{y}\vert \mathbf{X})=\mathbf{X\beta},
\end{eqnarray*}\]</span></p>
<p>i.e., the systematic
component of our linear regression model describes the conditional
expectation.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> If, in addition, we assume the errors to be normally
distributed, inference based on <span class="math inline">\(t\)</span> and <span class="math inline">\(F\)</span> tests is exact.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p>We could try to apply the least squares principle from above to a binary
response variable <span class="math inline">\(y_i\)</span> where the categories are coded with numerical
values <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. And to follow the argumentation it is useful to
think of the linear regression model in terms of the conditional
expectation function as specified above. For a binary variable
(Bernoulli variable) where <span class="math inline">\(p\)</span> is denoted as the &quot;success probability&quot;
we know that the expectation is
<span class="math inline">\(E(y_i) = 0 \cdot P(y_i=0) + 1 \cdot P(y_i=1) = P(y_i=1) =p_i\)</span>, i.e.,
the expectation equals the success probability. And due to
<span class="math inline">\(Var(y_i) = E(y_i - E(y_i))^2 = E(y_i^2) - E(y_i)^2\)</span> we get
<span class="math inline">\(Var(y_i) = p_i - p_i^2 = p_i(1-p_i)\)</span>. If we were to use this in a
linear regression we would have
<span class="math inline">\(P(y = 1|\mathbf{x}) = E(y_i\vert \mathbf{x_i})=\mathbf{x_i&#39;\beta}\)</span> and
consequently
<span class="math inline">\(Var(y_i\vert \mathbf{x_i})=\mathbf{x_i&#39;\beta}(1-\mathbf{x_i&#39;\beta})\)</span>.
The problem with this specification is that
<span class="math inline">\(\mathbf{x_i&#39;\hat{\mathbf{\mathbf{\beta}}}}\)</span> (our linear predictor) is
not restricted and can lead to nonsense predictions, i.e., predictions
that are outside of the admissible range of <span class="math inline">\([0, 1]\)</span> and is therefore
not well suited for modeling the success probability of our response
variable <span class="math inline">\(y_i\)</span> for a specific individual or entity given a set of
covariates <span class="math inline">\(x_{ji}\)</span>. In addition,
<span class="math inline">\(Var(y_i\vert \mathbf{x_i})=\mathbf{x_i&#39;\beta}(1-\mathbf{x_i&#39;\beta})=p_i(1-p_i)\)</span>
shows that if <span class="math inline">\(p_i\)</span> differs across individuals (depending on our linear
predictor) then also our variance is not constant but is different for
every individual <span class="math inline">\(i\)</span>, i.e. we have heteroscedasticity.</p>
<p>To overcome the difficulties, a different estimation technique is
applied, namely Maximum Likelihood Estimation (MLE). In contrast to OLS,
in MLE the starting point is a parametric distribution of the response
variable (or the error term) where the parameters are specified as a
function of the exogenous variables. By modeling the probability
distribution of a response <span class="math inline">\(\mathbf{y}\)</span> conditional on regressors
<span class="math inline">\(\mathbf{X}\)</span> the emphasis is shifted away from the conditional
expectation function as in the OLS-framework towards the <strong>conditional
probability function</strong>. This procedure can be applied to a broader class
of distributions within the framework of Generalized Linear Models
(GLMs) which are an extension of the classical linear regression model
to accommodate both non-normal response distributions and
transformations to linearity. This framework allows a unified treatment
of statistical methodology for several important classes of models where
the response variables have an error distribution out of the class of
the exponential family which incorporates the Bernoulli, Binomial,
Poisson, Gamma or inverse Normal distribution etc. The GLM thereby
allows the linear predictor to be related to the response variable via a
link function and allows the magnitude of the variance of each
observation to be a function of its predicted value.<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a></p>
<div id="glms" class="section level4">
<h4>GLMs</h4>
<p>For GLMs the conditional distribution of <span class="math inline">\(y_i\vert \mathbf{x_i}\)</span> is
given by a density of the following type</p>
<p><span class="math display">\[\begin{eqnarray*}
f(y; \theta, \phi)=exp\left(\frac{y\theta-b(\theta)}{\phi}+c(y, \phi)\right)
\end{eqnarray*}\]</span></p>
<p>where <span class="math inline">\(b(.)\)</span> and <span class="math inline">\(c(.)\)</span> are known functions and <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\phi\)</span> are
the so-called canonical and dispersion parameter.<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a> By appropriately
choosing <span class="math inline">\(b(.)\)</span> and <span class="math inline">\(c(.)\)</span> it is possible to generate the different
distributions (normal, binomial, Poisson, …). <br/>
To generate a regression model based on this exponential family one
further states the conditional expectation to be
<span class="math inline">\(\mu_i=E(y_i\vert \mathbf{x_i})\)</span> and the linear predictor to be
<span class="math inline">\(\eta_i=\mathbf{x_i&#39;\beta}\)</span> which are connected via a monotone
transformation <span class="math inline">\(g(\mu_i)=\eta_i=\mathbf{x_i&#39;\beta}\)</span> where <span class="math inline">\(g(.)\)</span> is
called <em>link function</em>.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> And because <span class="math inline">\(g(.)\)</span> is assumed to be a
monotonic function it is invertible and we can rearrange to get
<span class="math inline">\(\mu_i = g^{-1}(\eta_i) = g^{-1}(\mathbf{x_i&#39;\beta})\)</span>. Furthermore,
<span class="math inline">\(\theta\)</span> is also an invertible function of <span class="math inline">\(\mu\)</span> and it can be shown
that <span class="math inline">\(E(y_i\vert \mathbf{x_i})=\mu_i=b&#39;(\theta_i)\)</span>, which could be
rearranged to <span class="math inline">\(\theta_i=(b&#39;)^{-1}(\mu_i)\)</span>, and
<span class="math inline">\(Var(y_i\vert \mathbf{x_i})=\phi b&#39;&#39;(\theta_i)\)</span>. We see that the
canonical parameter <span class="math inline">\(\theta\)</span> is connected to our conditional expectation
<span class="math inline">\(\mu\)</span> via the function <span class="math inline">\(b(.)\)</span> and that <span class="math inline">\(E\)</span> and <span class="math inline">\(Var\)</span> are not completely
separated but the variance is, in the general case, a function of <span class="math inline">\(\phi\)</span>
and <span class="math inline">\(\theta\)</span>, therefore the variance is also a function of our
expectation <span class="math inline">\(\mu\)</span> which is often written down as
<span class="math inline">\(Var(y_i\vert \mathbf{x_i})=\phi b&#39;&#39;(\theta(\mu))\)</span> where
<span class="math inline">\(V(\mu)=b&#39;&#39;(\theta(\mu))\)</span> is called the variance function. If we combine
all of the above we see that</p>
<p><span class="math display">\[\begin{eqnarray*}
\theta_i = (b&#39;)^{-1}(\mu_i)=(b&#39;)^{-1}(g^{-1}(\mathbf{x_i&#39; \beta}))
\end{eqnarray*}\]</span></p>
<p>where we see that we have multiple transformations between the linear
predictor, the conditional expectation and the canonical parameter. In
the linear regression model we have $ =_i=_i=_i $.</p>
</div>
<div id="a-glm-for-binary-responses" class="section level4">
<h4>A GLM for Binary Responses</h4>
<p>To pass these concepts on to a Bernoulli-distributed random variable <span class="math inline">\(y\)</span>
which takes the value 1 with success probability <span class="math inline">\(p\)</span> and the value 0
with probability <span class="math inline">\((1-p)\)</span> we can, as a first step, simply write down the
probability mass function which is</p>
<p><span class="math display">\[\begin{eqnarray*}
f(y;\mu) = \mu^y (1-\mu)^{1-y}\!\quad \text{for }y\in\{0,1\}.
\end{eqnarray*}\]</span></p>
<p>This mass function<a href="#fn6" class="footnote-ref" id="fnref6"><sup>6</sup></a> can be rewritten to</p>
<p><span class="math display">\[\begin{eqnarray*}
f(y; \mu) &amp; = &amp; \mu^{y} (1-\mu)^{1-y} \\
          &amp; = &amp; exp \left \{y \cdot log \left ( \frac{\mu}{1-\mu} \right) + log(1 - \mu) \right \}
\end{eqnarray*}\]</span></p>
<p>In this representation we see that <span class="math inline">\(\phi=1\)</span> and that <span class="math inline">\(c(y, \phi)\)</span> is
also not part of it. Comparing the remaining terms to the general
representation of GLMs we see that
<span class="math inline">\(\theta=log\left(\frac{\mu}{1-\mu}\right)\)</span>, i.e. our canonical parameter
<span class="math inline">\(\theta\)</span> is a function of our conditional expectation <span class="math inline">\(\mu\)</span>. Here,
<span class="math inline">\(\theta\)</span> is said to be the logit transform of <span class="math inline">\(\mu\)</span>. Solving for <span class="math inline">\(\mu\)</span>
we see that <span class="math inline">\(\mu=\frac{e^\theta}{1+e^\theta}\)</span> and we further get
<span class="math inline">\(b(\theta)=-log\left(1-\frac{e^\theta}{1+e^\theta}\right)=...=log(1+e^\theta)\)</span>.
From this we can derive
<span class="math inline">\(b&#39;(\theta) = \frac{1}{1 + exp(\theta)} \cdot exp(\theta) = \mu\)</span> and
after some algebra also the variance function
<span class="math inline">\(V(\mu) = b&#39;&#39;(\theta(\mu)) = \mu(1 - \mu)\)</span>. By this we can represent the
variance function as a function of <span class="math inline">\(\mu\)</span> instead of our canonical
parameter <span class="math inline">\(\theta\)</span><a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>. So we see, that in the Bernoulli-model it holds
that <span class="math inline">\(E[y_i\vert \mathbf{x_i}]=\mu_i=b&#39;(\theta)\)</span> and that
<span class="math inline">\(Var[y_i\vert \mathbf{x_i}]=\phi \cdot V(\mu_i)=\phi \cdot b&#39;&#39;(\theta(\mu))=\mu_i(1-\mu_i)\)</span>,
what is exactly what we postulated above. Here we clearly see how the
conditional expectation <span class="math inline">\(\mu\)</span> influences our variance.<a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a> <br/></p>
<p>In general, each response distribution allows a variety of link functions to connect
the mean with the linear predictor. In the case of binary dependent
variables, the two most widely used links are the <em>logit</em> and the
<em>probit</em> link<a href="#fn9" class="footnote-ref" id="fnref9"><sup>9</sup></a>. For each response distribution the link function
<span class="math inline">\(g=(b&#39;)^{-1}\)</span> for which we get <span class="math inline">\(\theta \equiv \eta\)</span> is called the
<em>canonical link</em>. In case of binary responses the two most widely used
links are the canonical <em>logit</em> or <em>probit</em> link. From above we see that
<span class="math inline">\(\theta=(b&#39;)^{-1}(\mu)=log\left(\frac{\mu}{1-\mu}\right)\)</span> and so the
link function <span class="math inline">\(g=(b&#39;)^{-1}\)</span> for which we get <span class="math inline">\(\theta \equiv \eta\)</span> is
exactly <span class="math inline">\(log\left(\frac{\mu}{1-\mu}\right)\)</span> because then
<span class="math inline">\(\theta = (b&#39;)^{-1} (g^{-1} (\mathbf{x&#39; \beta})) = \mathbf{x&#39; \beta} = \eta\)</span>.
By setting <span class="math inline">\(log\left(\frac{\mu}{1-\mu}\right)=\eta\)</span> we also see that
<span class="math inline">\(\frac{\mu}{1-\mu}=exp(\eta)\)</span> and further that
<span class="math inline">\(\mu = \frac{\exp(\eta)}{1+exp(\eta)}\)</span>.</p>
<p>In the paragraph above we rolled up the field from behind in an attempt
to show how the relevant quantities naturally arise from a Bernoulli
distribution. In practice, this is of course known and therefore one can
immediately summarize the relevant results as follows: a binary
regression model links the conditional probability that we want to model
<span class="math inline">\(\mu_i = P(y_i=1\vert \mathbf{x_i})=E(y_i\vert\mathbf{x_i})\)</span> to the
linear predictor <span class="math inline">\(\eta_i\)</span> via a function <span class="math inline">\(g^{-1}(.)\)</span> which is a strict
monotonic increasing distribution function so that
<span class="math inline">\(g^{-1}(\eta) \in [0, 1]\)</span> always holds and we have</p>
<p><span class="math display">\[\begin{eqnarray*}
\mu_i=g^{-1}(\eta_i)=g^{-1}(\mathbf{x_i&#39;\beta})
\end{eqnarray*}\]</span></p>
<p>By means of the
inverse function <span class="math inline">\(g=(g^{-1})^{-1}\)</span> it holds that
<span class="math inline">\(g(\mu_i)=\eta_i=\mathbf{x_1&#39;\beta}\)</span>. In the GLM-terminology <span class="math inline">\(g^{-1}(.)\)</span>
is called <em>response function</em> and <span class="math inline">\(g\)</span> is called canonical <em>link
function</em>. It is important to note that the response function
<span class="math inline">\(g^{-1}(.)\)</span> should be chosen such that the linear predictor is mapped to
the unit interval, i.e. <span class="math inline">\(g^{-1}:\mathbf{x_i&#39;\beta}\rightarrow[0, 1]\)</span> and
the link function accordingly maps the unit interval to the real line,
i.e. <span class="math inline">\(g:[0, 1] \rightarrow \mathbb{R}\)</span>. The logit-model then arises by
choosing the cumulative distribution function (CDF) of the logistic
distribution</p>
<p><span class="math display">\[\begin{eqnarray*}
P(y_i = 1) = \mu=g^{-1}(\eta)=\frac{exp(\eta)}{1+exp(\eta)} = \Lambda(\eta)
\end{eqnarray*}\]</span></p>
<p>that ensures that that <span class="math inline">\(\mu\)</span> is restricted to the interval <span class="math inline">\([0,1]\)</span> and
the corresponding <em>logit-link</em> function then is</p>
<p><span class="math display">\[\begin{eqnarray*}
g(\mu)=log\left(\frac{\mu}{1-\mu}\right)=\eta=\mathbf{x_i&#39;\beta}.
\end{eqnarray*}\]</span></p>
<p>By
this one generates a linear model in the log-odds (the ratio of success
probability to the probability of failure) <span class="math inline">\(log(\mu/(1-\mu))\)</span> and
transforming by taking <span class="math inline">\(exp()\)</span> yields</p>
<p><span class="math display">\[\begin{eqnarray*}
\frac{\mu}{1-\mu}=exp(\mathbf{x_i&#39;\beta})
\end{eqnarray*}\]</span></p>
<p>which shows how the
covariates influence the odds <span class="math inline">\(\mu/(1-\mu)\)</span>. <br/>
Similarly also other CDFs <span class="math inline">\(F(.)\)</span> with <span class="math inline">\(F: \mathbb{R} \rightarrow [0, 1]\)</span>
can be used as the inverse link function, the most popular being</p>
<p><span class="math display">\[\begin{eqnarray*}
g(\mu)=\Phi^{-1}(\mu)
\end{eqnarray*}\]</span></p>
<p>where <span class="math inline">\(\Phi^{-1}\)</span> is the inverse CDF of the
standard normal distribution and therefore this model entered the
literature as the <em>probit</em> model. In terms of the normal distribution we
would get</p>
<p><span class="math display">\[\begin{eqnarray*}
P(y_i = 1) &amp; = &amp; \mu = g^{-1}(\eta) = \Phi(\mathbf{x_i&#39;\beta}) = \Phi(\eta)\\
                &amp; = &amp; \int_\infty^{\mathbf{x_i&#39;\beta}} \! \phi(z) dz = \int_\infty^{\mathbf{x_i&#39;\beta}} \! \frac{1}{2\pi} exp\left( \frac{-z^2}{2} \right) dz
\end{eqnarray*}\]</span></p>
<p>As can be seen in
the Figure below, the logistic and normal CDF, i.e., the inverse
link functions (response functions) that map the linear predictor to the
unit interval are very similar. They are both symmetric around zero but
the logistic CDF somewhat slower approaches 1 and 0 for
<span class="math inline">\(\eta \rightarrow +/- \infty\)</span> respectively.</p>
<p><img src="/img/logprob.png" /></p>
<p><em>Inverse Logit and Probit functions.</em></p>
<p>This emerges because the logistic distribution has somewhat heavier
tails. And because the logistic CDF has a variance of <span class="math inline">\(\pi^2/3\)</span> (and not
1 like the standard normal distribution) it has to be compared with a
rescaled normal CDF <span class="math inline">\(\Phi\)</span> with a variance of <span class="math inline">\(\sigma^2=\phi^2/3\)</span>. In the
Figure above one can see that the Logit- and the (adjusted)
Probit response function are very similar. Statistical analyses with
Logit- and Probit-models therefore always lead to very similar results
for the predicted probabilities. The rescaling of the parameters is
thereby conserved whereby the coefficients of a Logit-models differ
approximately by a factor of <span class="math inline">\(\sigma=\pi\sqrt{3} \approx 1.814\)</span>, but the
estimated probabilities are very similar.<br/>
By specifying all of the above and further assume independence (which is
often natural to assume for cross-section data), we can specify the full
likelihood where the parameters are connected to the linear predictor
via the link-function <span class="math inline">\(g(\mu)=\eta\)</span> and it is possible to estimate the
model via maximum likelihood. For this it is important to note that the
linear predictor <span class="math inline">\(\eta_i=\mathbf{x_i&#39;\beta}\)</span> depends on <span class="math inline">\(\beta\)</span> and
influences our conditional expectation <span class="math inline">\(\mu_i\)</span> and further our canonical
parameter <span class="math inline">\(\theta\)</span>.<a href="#fn10" class="footnote-ref" id="fnref10"><sup>10</sup></a> In general, there is no analytic solution to
this expression and is therefore typically estimated by <em>iterative
weighted least squares</em> which is a version of the Fisher scoring adapted
to GLMs.</p>
</div>
<div id="latent-variable-approach" class="section level4">
<h4>Latent Variable Approach</h4>
<p>So far we have argued that we want to model probabilities therefore,
statistically, it makes sense to map our linear predictor to the unit
interval. Apart from these considerations there are of course also
economic/behavioural justifications for why we use these specifications
and the corresponding link functions. Before proceeding to these
considerations we want to briefly discuss the <em>latent variable
approach</em>. <br/>
The idea is that the binary outcomes we observe are generated by an
unobservable underlying (latent) variable <span class="math inline">\(y_i\)</span> that captures the
propensity to have a &quot;success&quot; (i.e., outcome <span class="math inline">\(1\)</span>) or not. Only if
this latent variable is larger than some threshold <span class="math inline">\(t\)</span>, we observe a
success. This can be written down via
<span class="math inline">\(y_i^* = \mathbf{x_i^\beta} + \epsilon_i\)</span> where the linear predictor
models the mean of the latent variable. But what we actually observe is
only whether the latent variable exceeded some certain threshold., i.e.</p>
<p><span class="math display">\[\begin{eqnarray*}
y_i = I(y_i^* \geq t) = 1  \text{ if } y_i^* \geq t \text{ and } 0 \text{ otherwise }
\end{eqnarray*}\]</span></p>
<p>Most frequently the threshold value <span class="math inline">\(t\)</span> is chosen to be <span class="math inline">\(0\)</span> because
choosing a different threshold would only affect the intercept in the
estimations. The probability to have a success, i.e., that <span class="math inline">\(y_i = 1\)</span> for
a given set of explanatory variables can then be written down as the
corresponding value of the CDF evaluated at <span class="math inline">\(\mathbf{x_i&#39;\beta}\)</span>. This
can be derived via <a href="#fn11" class="footnote-ref" id="fnref11"><sup>11</sup></a></p>
<p><span class="math display">\[\begin{eqnarray*}
P(y_i = 1 | \mathbf{x_i&#39;}) = \mu_i &amp; = &amp; P(y_i^* \geq 0| x_i) = P(\mathbf{x_i&#39;\beta} + \epsilon_i \geq 0 | \mathbf{x_i})\\
         &amp; = &amp; P(\epsilon_i \geq \mathbf{-x_i&#39;\beta} | \mathbf{x_i}) = P(\epsilon \leq \mathbf{x_i&#39;\beta}| \mathbf{x_i})\\
         &amp; = &amp; F(\mathbf{x_i&#39;\beta)}
\end{eqnarray*}\]</span></p>
<p>This shows that by looking at latent variables where the
latent variable is explained by a mean which is the linear predictor plus
some random error term we get incidental decisions (<span class="math inline">\(0, 1\)</span>) where the
probability depends on the linear predictor and the distribution of the
remaining error term around the latent variable. And then, by assuming
that <span class="math inline">\(-\epsilon\)</span> has a logistic distribution brings us to the logit
model and assuming <span class="math inline">\(-\epsilon\)</span> to be normally distributed brings us to
the probit model.</p>
</div>
<div id="mle" class="section level4">
<h4>MLE</h4>
<p>Under independence, the likelihood is the product of the individual
densities (i.e. the joint density, given the data) as a function of the
parameter vector<a href="#fn12" class="footnote-ref" id="fnref12"><sup>12</sup></a> which can be written down as <a href="#fn13" class="footnote-ref" id="fnref13"><sup>13</sup></a></p>
<p><span class="math display">\[\begin{eqnarray*}
L(\theta)=L(\theta; \mathbf{y}) = \prod_{i=1}^n f(\theta; y_i).
\end{eqnarray*}\]</span></p>
<p>And because it is computationally easier to handle (and also numerically
more stable), in practice one generally uses a log-transformation (which
is monotone with respect to maximization) and writes down the
log-likelihood</p>
<p><span class="math display">\[\begin{eqnarray*}
l(\theta)=logL(\theta) = \sum_{i=1}^n log f(\theta; y_i).
\end{eqnarray*}\]</span></p>
<p>where,
under independence of our sample, products are turned into
computationally simpler sums. Then the maximum likelihood estimator
(MLE) is the parameter estimator <span class="math inline">\(\hat{\mathbf{\theta}}\_{ML}\)</span> that
maximizes the log-likelihood, i.e.
<span class="math inline">\(\hat{\mathbf{\theta}}\_{ML}=\arg\max_\theta L(\theta) = \arg\max_\theta l(\theta).\)</span>
And because the MLE depends on the specific observations (our sample)
they can be considered as random variables. For inference it is then
usual to look at Wald tests, Lagrange multiplier tests (LM) or
Likelihood ratio (LR) tests. If GLMs are correctly specified, they
inherit all properties of correctly specified ML estimation: efficiency,
asymptotic normality and consistency.<br/>
Here we will not in detail discuss Maximum Likelihood theory but only
write down the relevant Bernoulli likelihood that is used in estimation
for binary response models and assume that the necessary regularity
conditions and correct specifications of the model guarantee that our
MLE is consistent, asymptotically normal and efficient.<br />
Assuming that the sample is i.i.d the joint distribution for the binary
choice model can be written down as</p>
<p><span class="math display">\[\begin{eqnarray*}
Pr(\mathbf{y}|\mathbf{X}) = \prod_{i, y_i = 1} [F(\mathbf{x_i&#39;\beta}] \prod_{i, y_i = 0} [1 - F(\mathbf{x_i&#39;\beta})]
\end{eqnarray*}\]</span></p>
<p>and because the observed <span class="math inline">\(y_i\)</span>’s are the outcome of Bernoulli
experiments the likelihood for a sample of <span class="math inline">\(n\)</span> is then</p>
<p><span class="math display">\[\begin{eqnarray*}
L(\beta | \mathbf{y, X} = \prod_{i=1}^n [F(\mathbf{x_i\beta}]^{y_i} [1 - F(\mathbf{x_i&#39;\beta}]^{1-y_i}
\end{eqnarray*}\]</span></p>
<p>where <span class="math inline">\((.)\)</span> again stands for either the CDF of the normal or the
logistic distribution. The parameters of this model are then estimated
by numerical methods (e.g. Newton-Raphson Algorithm).</p>
</div>
</div>
<div id="application-german-health-care-usage-gsoep" class="section level1">
<h1>Application: German Health Care Usage (GSOEP)</h1>
<p>For our empirical application we want to make use of a dataset that is
about German health care usage and originates from the German
Socioeconomic Panel Survey (GSOEP). The sample covers the years 1984-95
and was used in a study by <span class="citation">@riphahn:03</span> that investigated whether the
presence of private insurance had an effect on the utilization (counts)
of physician visits and hospital visits. The dataset is an unbalanced
panel of 7,293 households and among the variables reported are household
income with numerous additional sociodemographic variables including
<code>age</code>, <code>gender</code>, <code>education</code> etc. The number of observations varies from
one to seven (1,525; 1,079; 825; 926; 1,311; 1,000; 887) with a total
number of 27,326 observations.<a href="#fn14" class="footnote-ref" id="fnref14"><sup>14</sup></a></p>
<p>The dataset can be used to assess various research questions with
various econometric models (count data models like Poisson regression,
zero-inflation models or hurdle-models; binary choice models etc.).
Since we want to model binary choices, our variable of interest
(dependent variable) for this application is a binary variable <span class="math inline">\(y\)</span> that
indicates whether an individual visited a doctor <em>at all</em> or not. That
is, from the potential information of counts we have in the dataset our
variable <span class="math inline">\(y\)</span> takes on only the value <span class="math inline">\(0\)</span> for <span class="math inline">\(docvis=0\)</span> and <span class="math inline">\(1\)</span> for
<span class="math inline">\(docvis=1\)</span>. From the various socioeconomic variables (covariates) we
decided to use only <code>gender</code>, <code>age</code>, <code>age2</code>, <code>hhinc</code>, <code>hhkids</code>, <code>educ</code>,
<code>married</code> and <code>working</code> for the regressions.</p>
<p><img src="/img/visits.png" />
<em>Count variable of doctor visits (left), binary variable (right).</em></p>
<p>In the Figure above we can see the count variable <code>docvis</code> in
contrast to the binary variable <code>doc</code>. The amount of observations that
visited a doctor at least once (i.e. <span class="math inline">\(y&gt;0\)</span>) is larger compared to
observations for which we have <span class="math inline">\(y=0\)</span>. There are a total of 10,135
observations that did <em>not</em> visit a doctor in contrast to 17,191 that
did. In the next Figure elow we have also plotted the variable <code>healthsat</code>
across gender for each level (ranging from 0 - 10). However, we have
decided to not use this variable in the regressions.<br/></p>
<p><img src="/img/healthsat.png" />
<em>Health satisfaction, coded 0 (low) - 10 (high) by gender.</em></p>
<p>Apart from the above, we can also have a look at bivariate relationships
between our response and some of the regressors. In particular assessing
the relationship between <code>educ</code> and <code>age</code> with our dependent variable in our
Figure below, we see that with increasing education the
tendency to visit a doctor decreases while with increasing age there
seems to be a quadratic effect so that first the tendency to visit a
doctor slightly decreases and then takes up again after the
mid-fourties.</p>
<p><img src="/img/educage.png" />
<em>Bivariate relationship between educ and age with doctor visits.</em></p>
<div id="models" class="section level4">
<h4>Models</h4>
<p>As mentioned above, the dataset allows a vast array of models to be
estimated. Within the binary choice models we could also make use of the
panel structure of the data at hand. But due to time restrictions in
preparing this project we restricted ourselves to the pooled logit and
probit estimation and can hopefully add the binary models to this
project anytime soon. Both models are estimated with the covariates
<code>gender</code>, <code>age</code>, <code>age2</code>, <code>hhinc</code>, <code>hhkids</code>, <code>educ</code>, <code>married</code> and
<code>working</code>.<br/>
The Table below
reports the regression output of these two models. Looking solely at the
sign of the coefficients we see that for <code>gender: female</code> we have a
higher probability to visit a doctor, for <code>age</code> there are two effects
(age and age2) where the coefficient for <code>age</code> is negative and the one
for <code>age2</code> positive, indicating that the probability to visit a doctor
first decreases with age and then goes up again (though the effects are
fairly small). For <code>hhinc</code> the effect is almost zero and not
significant, and for <code>hhkids: yes</code>, <code>educ</code>, <code>married: yes</code> and
<code>working: yes</code> the effects are significantly negative (though only on a
5% significant level for <code>working: yes</code>).</p>
<p><img src="/img/BinaryChoice.png" />
<em>Estimation results Logit and Probit.</em></p>
</div>
<div id="interpretation-of-parameters" class="section level4">
<h4>Interpretation of Parameters</h4>
<p>Between logit and probit models, often logit-models are preferred
because, besides the latent variable approach which they have both in
common, the logit-transformation has the advantage that it can be
regarded as a linear model on the log-odds scale, as we have stated
above. By taking expectations, i.e. <span class="math inline">\(exp(\mathbf{x_i&#39;\beta})\)</span>, we get a
model on the odds scale, therefore the coefficients of logistic
regressions can be interpreted in terms of <em>odds</em>. This approach makes
it easy to compare two groups of subjects if they differ only for the
<span class="math inline">\(lth\)</span> regressor by one unit which then results in <span class="math inline">\(exp(\beta_l)\)</span> and
immediately states the odds ratio, which is the ratio of the respective
odds between the two groups. Relative changes in the odds ratio are then
simply <span class="math inline">\(exp(\beta_l) - 1\)</span>. By this we can immediately interpret
<span class="math inline">\(exp(\beta)-1\)</span> as changes on the odds-scale by looking at the
coefficients from our logit-regression. For small <span class="math inline">\(\beta_l\)</span> we further
have <span class="math inline">\(exp({\beta_l})-1 \approx \beta_l\)</span>, c.p. For the probit model
instead, a direct interpretation of the coefficients like in the logit
model is not possible. Still these considerations don’t tell us
something about probabilities which we actually want to model.<a href="#fn15" class="footnote-ref" id="fnref15"><sup>15</sup></a> For
the remainder we will foremost focus on the interpretation of the
logit-model (due to its interpretation in terms of odds) noting that in
terms of predicted probabilities the two models are virtually
equivalent.</p>
<p>Taking a step towards probabilities and how they are affected by changes
in the regressors, things become slightly more involved than in the CLM.
First of all, the estimated coefficients can be used to calculate the
respective probabilities for a certain linear predictor <span class="math inline">\(\eta\)</span>, ie. for
every individual in the dataset with certain characteristics there
exists a corresponding (success) probability that can be computed by
simply evaluating
<span class="math inline">\(\hat{\mathbf{P}}r(y_i|\mathbf{x_i&#39;}) = F(\mathbf{x_i&#39;\hat{\mathbf{\mathbf{\beta}}}})\)</span>.
This can be, for example, used to calculate an average success
probability across all individuals (which is of course not very
informative as it summarizes a lot of information in the dataset). In
our case the mean of the fitted probabilities (predicted probabilities)
is about 0.63 (for both logit and probit). In R the <code>predict</code>-function
can be used to compute predictions of the response
<span class="math inline">\(\hat{\mathbf{\mu}}_i = \hat{\mathbf{P}}r(y_i|\mathbf{x_i&#39;})\)</span> where the
default is the prediction of the linear predictor. Comparing, for
example, an individual with
<code>gender=male, age=44, hhinc=3500, kids=yes, educ=18, married=yes, working=yes</code>
to a person with the exact same characteristics <em>except</em> that we set
gender=female we get a predicted probability to visit a doctor of 0.4731
as compared to 0.6123, respectively.<a href="#fn16" class="footnote-ref" id="fnref16"><sup>16</sup></a> The <code>predict</code>-function thereby
takes the data and multiplies the vector of coefficients with the design
matrix. By setting <code>type = &quot;response&quot;</code> the linear predictors are then
automatically sent through the inverse logit link function
<span class="math inline">\(\Lambda(\eta)\)</span> that evaluates the corresponding probability.<a href="#fn17" class="footnote-ref" id="fnref17"><sup>17</sup></a> From
these calculated probabilities it then possible to get back to the odds
by plugging the calculated probabilities into the logit-link function or
<span class="math inline">\(exp(logit-link)\)</span> function for the log-odds). So for a male person with
the above stated characteristics the probability to visit a doctor are
0.8987 times the probability to not visit a doctor whereas for a female
person the odds to visit a doctor are 1.578 times the probability to not
visit a doctor.</p>
<p>For our odds-ratio (as outlined in the previous paragraph) we would get
<span class="math inline">\(exp(0.564) = 1.758\)</span>, meaning that the probability of a female person
with a specific combination of covariates to visit a doctor is 1.758
times higher than the odds of a male person with exactly the same
covariate values (i.e., the regressors just differ by the <span class="math inline">\(lth\)</span>
regressor by one unit!). And the corresponding relative change in the
odds ratio is then <span class="math inline">\(0.7577 \approx 76\%\)</span>, meaning that the odds to visit
a doctor increase by 70% by changing the <span class="math inline">\(lth\)</span> regressor from <code>male</code> to
<code>female</code>.<a href="#fn18" class="footnote-ref" id="fnref18"><sup>18</sup></a> Similarly we can state that if education increases by one
unit we expect the odds to visit a doctor to decrease by about
<span class="math inline">\(exp(-0.023) - 1 = -0.02274 = -2.274\%\)</span>, all else equal. Similarly
having kids under 16 years in the household decreases the odds to visit
a doctor by about 15%, being married increases them by about 14% and
working decreases them by about 7% respectively. One feature of the
calculation and report of odds is that they are still less well
interpretable than probabilities (which are our primary interest).<br />
Alternatively it would also be possible to calculate the probability at
the mean regressors which results in
<span class="math inline">\(\hat{\mathbf{P}}r(y_i|\mathbf{\hat{\mathbf{\mathbf{x}}}_i&#39;}) = F(\mathbf{\hat{\mathbf{\mathbf{x}}}_i&#39;\hat{\mathbf{\mathbf{\beta}}}})\)</span>.
In our case the predicted probability at the mean regressors across the
sample are 0.46 (logit) and 0.68 (probit) respectively.</p>
<p>In usual linear models the expectation <span class="math inline">\(E(y_i|\mathbf{x_i})\)</span> that we
want to model is a <em>linear function</em> in the parameters, therefore every
single parameter could be directly interpreted as a marginal effect on
the expectation, ceteris paribus. An important difference of binary
choice models to the CLM is that partial derivatives of the expectation,
i.e. marginal effects, are not constant because they not only depend on
the parameter under consideration but on the entire linear predictor
that, of course, differs from person to person. This means that marginal
effects result in</p>
<p><span class="math display">\[\begin{eqnarray*}
\frac{\partial P(y_i = 1|\mathbf{x_i&#39;})}{\partial x_h} = \frac{\partial F(\mathbf{x_i&#39;\beta})}{\partial\mathbf{x_h}} = f(\mathbf{x_i&#39;\beta}) \cdot \beta_h
\end{eqnarray*}\]</span></p>
<p>where <span class="math inline">\(f(.)\)</span> now stands for the derivative of either the logistic or
normal CDF, i.e. the corresponding <em>density</em>. The factor <span class="math inline">\(f(.)\)</span> is
always positive, therefore the sign of the corresponding <span class="math inline">\(f\beta_h\)</span>
directly shows in which direction the expectation changes. Due to the
non-linearity he <em>size</em> of a marginal effect crucially depends on the
actual starting value we are considering when assessing the effect of
the change of the <span class="math inline">\(lth\)</span> regressor on the predicted probability.
Depending on the individual and its characteristics (that is
incorporated in the linear predictor), the marginal effect can be larger
or smaller. Due to this, it is common to report &quot;typical&quot; effects,
namely <em>average</em> marginal effects, similar to above where we reported
average predicted probabilities and predicted probabilities at mean
regressors. These can be calculated either by taking the mean over all
observations (i.e. calculate the marginal effect for each observation
and than take the mean) <em>or</em> calculate the marginal effect at the mean
of the regressors. In the former case we average across the
probablity-scale, in the later across the regressor-scale, therefore
also the results will differ.<br/>
Because average probabilities as well as average marginal effects always
merge a lot of information in one single number, their usefulness in
terms of representation of the information in the data is debatable.
Therefore it is often better to evaluate marginal probability effects by
comparing groups in the data (here: married vs. not married, working
vs. not working). Another issue is that software packages to compute
marginal effects often don’t compute them correctly when there are
interactions or polynomials in the model.</p>
<p>We have seen that both, odds and average (marginal) probabilities
(effects) are either difficult to interpret or not very representative
for the information in the dataset. Therefore, another very helpful tool
to assess the information in the data is to fix all but the <span class="math inline">\(lth\)</span>
regressor at their mean and then evaluate the marginal probability
effects for the steps in the <span class="math inline">\(lth\)</span> regressor or directly plot the
predicted probability against the entire range of values of the <span class="math inline">\(lth\)</span>
regressor. We want to show here the latter for <code>educ</code> which are
so-called &quot;effect-plots&quot;. Such plots can be either generated by going
step by step through the respective calculations, setting up a
design-matrix where all but one regressor are fixed to a &quot;typical&quot;
value we want to investigate and then let the <span class="math inline">\(lth\)</span> regressor &quot;run&quot;
and see how the predicted probabilities change. That is, we look how
changes in one regressor, everything else fixed, translate into effects
on probabilities. So instead of looking at marginal effects (which are
the derivative of our expectation = success probability) it is often
more convenient to directly look at the probabilities themselves. We
have done this to assess the probability to visit a doctor depending on
education while keeping all other values fixed to either their mean (for
numeric data) or a &quot;typical&quot; value (for dummies). This exercise was
then replicated for the same set of values except that we changed the
status of <code>working</code> from &quot;yes&quot; to &quot;no&quot; (red line in the plot). The
result can be seen in
Figure below where we see that if education runs
from 7 to 18, the probability to visit a doctor decreases from slightly
above 57% to about 51%. In between the function is almost linear. By
changing <code>working</code> from &quot;yes&quot; to &quot;no&quot; shifts our line so that
persons that don’t work have a higher probability (but still decreasing
for increasing education) across the entire range as compared to persons
that do work.<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a></p>
<p><img src="/img/educeffect.png" />
<em>Effect of education (everything else fixed at mean regressor); red line: changing “working”
from “yes” to “no”.</em></p>
<p>If we want to apply the procedure above to every covariate one at a time
we can use the <code>effects</code>-package in R that does exactly what we did
automatically and generates the respective effect-plots. For this it is
important to slightly adapt the function-call for the &quot;glm&quot; so that R
recognizes that <code>age</code> and <code>age2</code> belong together. The Figure below shows the resulting output.</p>
<p><img src="/img/alleffects.png" />
<em>Effect of education (everything else fixed at mean regressor); red line: changing “working”
from “yes” to “no”.</em></p>
<p>To read these plots it is important to recognize that in each plot of
the respective regressor all other covariates are fixed at their mean
(or &quot;typical&quot;) values. By this it is possible to isolate the effects
of covariates on the predicted probabilities and analyze them one at a
time. We see a clear difference in the probability to visit a doctor
regarding <code>gender</code>, a quadratic age-effect whereby the probability to
visit a doctor increases with age from about 37 onwards. Household
income (<code>hhinc</code>) reflects the insignificant effect we already saw in the
regression output whereas having kids that are younger than 16
(<code>hhkids</code>) interestingly corresponds to a slightly lower probability to
visit a doctor. Education has the negative (almost linear effect) we
already showed above and married individuals have, ceteris paribus, a
slightly higher probability to visit a doctor whereas working
individuals have a slightly lower probability to do so. Our results show
that the corresponding differences in probabilities are most pronounced
for the <code>age</code>-effect whereas for the other covariates the differences
are fairly moderate.<br />
We want to emphasize again that for our results so far we ignored the
panel-structure in the data. Our pooled logit and probit regressions are
therefore just a starting point for more sophisticated binary choice
panel data models.</p>
</div>
<div id="goodness-of-fit" class="section level4">
<h4>Goodness of Fit</h4>
<p>To assess the goodness of fit of our models there are various tools and
procedures available. One option is the so-called McFadden <span class="math inline">\(R^2\)</span>. In
comparison to linear models where the <span class="math inline">\(R^2\)</span> is a figure that reports the
proportion of the total variation in the dependent variable that is
explained by the model, this variance decomposition does not work for
logit/probit models. Instead, a substitute is a log-likelihood
comparison of the full model (with all regressors) with a constant-only
model but it cannot be interpreted as the <span class="math inline">\(R^2\)</span> in linear models. The
McFadden <span class="math inline">\(R^2\)</span> is thereby calculated as</p>
<p><span class="math display">\[\begin{eqnarray*}
0 \leq R_{McFadden}^2 = 1- \frac{logLik_{unrestricted}}{logLik_{restricted}} \leq 1.
\end{eqnarray*}\]</span></p>
<p>For the pseudo <span class="math inline">\(R^2\)</span> a value near one just indicates a better model fit
than a value near zero. This is because if our explanatory variables
have no explanatory power at all we should get
<span class="math inline">\(logLik_{unrestricted} = logLik_{restricted}\)</span> and the pseudo-<span class="math inline">\(R^2\)</span> would
then become 0. In the extreme case where the model perfectly explains
the dependent variable, the log-likelihood would be zero and the
pseudo-<span class="math inline">\(R^2\)</span> would be 1.<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a> In our case the corresponding values for
the McFadden-<span class="math inline">\(R^2\)</span>s are <span class="math inline">\(0.0368\)</span> for the logit and <span class="math inline">\(0.03661\)</span> for the
probit model.</p>
<p>Another helpful visual tool are so-called &quot;confusion matrices&quot; that
check the quality of the predictions in-sample. They are contingency
tables of observed vs. predicted values where by choosing some cutoff
probability <span class="math inline">\(c\)</span> calculated probabilities are considered a &quot;success&quot; or
a &quot;failure&quot;. The table summarizes correct predictions on the main
diagonal and incorrect predictions on the off-diagonal. These
off-diagonal values can either be &quot;false negatives&quot;, i.e. cases that
were falsely predicted to not visit a doctor while they actually did and
&quot;false positives&quot;, i.e. cases that were falsely predicted to visit a
doctor while they actually didn’t. Having a look at the
Table,
we see that a fairly large amount of cases could be predicted accurately
but unfortunately also the false negative rate is quite high with 8299
cases.<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a></p>
<p><img src="/img/confmat.png" />
<em>Confusion Table for the Logit Model.</em></p>
<p>The last thing we did was to perform a Likelihood-Ratio test that
suggests that our model is a significant improvement over a
constant-only model.</p>
</div>
</div>
<div id="appendix" class="section level1">
<h1>Appendix</h1>
<div id="r-code" class="section level4">
<h4>R-Code</h4>
<pre class="r"><code>#Econometrics Project, Binary Choice Models
#Marcel Kropp
#19.6.2014

dev.off()
## inverse logit and probit
x &lt;- -47:47/10
par(mar = c(6, 6, 3, 3), mfrow = c(1, 1))
#quartz()
plot(x, pnorm(x), type = &quot;l&quot;, lwd = 2,
     xlab = expression(eta), ylab = expression(g(eta)^-1),
     main = &quot;&quot;, col = &quot;brown1&quot;, cex.lab = 0.8, cex.axis = 0.8)
lines(x, plogis(x), lwd = 2, col = &quot;green4&quot;)
lines(x, pnorm(x, sd = dnorm(0)/dlogis(0)), lwd = 2, lty = 2)

legend(&quot;topleft&quot;, c(&quot;logistic&quot;, &quot;normal&quot;, &quot;normal (adj)&quot;),
       lty = c(1, 1, 2), lwd = 2,
       col = c(&quot;green4&quot;, &quot;brown1&quot;, &quot;black&quot;),
       bty = &quot;n&quot;, cex=0.8)




#############
## preliminaries            
#############
options(prompt = &quot;R&gt;&quot;, continue = &quot;+&quot;, width = 64,
        digits = 4, show.signif.stars = TRUE)
#set.seed(1071)
getwd()
setwd(&quot;/Users/...&quot;)



#############
## original dataset            
#############
#original dataset is rather huge;
#27,326 Observations
#1 to 7 years, panel
#7,293 households observed
#The number of observations for a certain individual ranges from 1 to 7.
#Frequencies are: 1=1525, 2=2158, 3=825, 4=926, 5=1051, 6=1000, 7=987

#Here we list the variables that can be found in the complete dataset;
#The variables we used for our analysis are in CAPITAL LETTERS.

#variables:
#   ID...........person ID-number
#   FEMALE.......female = 1; male = 0
#   YEAR.........year of observation
#   AGE..........age in years
#   hsat.........health satisfaction,
#                     coded 0 (low) - 10 (high);
#                     Note, this variable has 40 coding errors.
#                     Variable NEWHSAT below fixes them.
#   handdum......handicapped = 1; otherwise = 0
#   handper......degree of handicap in percent (0 - 100)
#   DOCTOR.......1 if Number of doctor visits &gt; 0; otherwise = 0             
#   hospital.....1 if Number of hospital visits &gt; 0; otherwise = 0   
#   HHNINC.......household nom. month. netincome in German marks/10000
#   HHKIDS.......children under age 16 in the household = 1; otherwise = 0
#   EDUC.........years of schooling
#   MARRIED......married = 1; otherwise = 0
#   haupts.......highest degree is Hauptschul degree = 1; otherwise = 0
#   reals........highest degree is Realschul degree = 1; otherwise = 0
#   fachhs.......highest degree is Polytechnical degree = 1; otherwise = 0
#   abitur.......highest degree is Abitur = 1; otherwise = 0
#   univ.........highest degree is university degree = 1; otherwise = 0
#   WORKING......employed = 1; otherwise = 0
#   bluec........blue collar employee = 1; otherwise = 0
#   whitec.......white collar employee = 1; otherwise = 0
#   self.........self employed = 1; otherwise = 0
#   beamt........civil servant = 1; otherwise = 0
#   DOCVIS.......number of doctor visits in last three months
#   hospvis......number of hospital visits in last calendar year
#   public.......insured in public health insurance = 1;
#                otherwise = 0
#   addon........insured by add-on insurance = 1; otherswise = 0

#Notes: In the applications in the text of Greene, the following additional
#variables are used:
#   numobs.......Number of observations for this person.
#                Repeated in each row of data.
#   NEWHSAT = hsat; 40 observations on HSAT recorded between
#                   6 and 7 were changed to 7.




#############
## read in the dataset            
#############

if(!file.exists(&quot;healthcare.rda&quot;)) {
  url &lt;- &quot;http://people.stern.nyu.edu/wgreene/Econometrics/healthcare.csv&quot;
  download.file(url, destfile = &quot;healthcare.csv&quot;)
  ##read in the data into R
  healthcare &lt;- read.csv(&quot;healthcare.csv&quot;, header = TRUE)

  ##adjust dataset
  healthcare &lt;- subset(healthcare, select = c(id, DOCTOR, DOCVIS, NEWHSAT,
                                   EDUC, HHNINC, HHKIDS, AGE,
                                   FEMALE, MARRIED, WORKING,
                                   YEAR))
  #retransform the income variable
  healthcare$HHNINC &lt;- (healthcare$HHNINC)*10000
  #View(healthcare)

  #further transformations; relabelling, appropriate types
  healthcare &lt;- with(healthcare, data.frame(
    &quot;id&quot; = id,
    &quot;doc&quot; = factor(DOCTOR, levels = c(0, 1),
                   labels = c(&quot;no&quot;, &quot;yes&quot;)),
    &quot;docvis&quot; = DOCVIS,
    &quot;hsat&quot; = NEWHSAT,
    &quot;educ&quot; = EDUC,
    &quot;hhinc&quot; = HHNINC,
    &quot;hhkids&quot; = factor(HHKIDS, levels = c(0, 1), labels = c(&quot;no&quot;, &quot;yes&quot;)),
    &quot;age&quot; = AGE,
    &quot;gender&quot; = factor(FEMALE, levels = c(0, 1), labels = c(&quot;male&quot;, &quot;female&quot;)),
    &quot;married&quot; = factor(MARRIED, levels = c(0, 1), labels = c(&quot;no&quot;, &quot;yes&quot;)),
    &quot;working&quot; = factor(WORKING, levels = c(0, 1), labels = c(&quot;no&quot;, &quot;yes&quot;)),
    &quot;year&quot; = YEAR
  ))
  ## save the file in .rda format
  save(healthcare, file = &quot;healthcare.rda&quot;)
} else {
  load(&quot;healthcare.rda&quot;)
}


#a few separate bivariate plots
dev.off()
plot(doc ~ married, data = healthcare)
plot(doc ~ hhkids, data = healthcare)
plot(doc ~ hhinc, data = healthcare)
plot(doc ~ gender, data = healthcare)
plot(doc ~ married, data = healthcare)
plot(doc ~ working, data = healthcare)
plot(doc ~ age, data = healthcare)


#our variable of interest is the binary choice:
#visited doctor (yes/no)
barplot(table(healthcare$doc),
        xlab = &quot;Individual visited doctor in past year&quot;,
        ylab = &quot;absolute frequency&quot;)

#comparison reveals of count (docvis) and binary variable (doc)
layout(matrix(c(1, 2), nrow = 1), widths = c(1, 0.5))
matrix(c(1, 2), nrow = 1)
par(mar = c(4.1, 4.1, 2, 2))
plot(table(healthcare$docvis), main = &quot;&quot;, xlab = &quot;count variable (visits)&quot;,
                         cex.lab = 0.7, cex.axis = 0.7, ylab = &quot;frequency&quot;)
#then we set the margins for the second graph
par(mar = c(4.1, 0.5, 2.1, 0.9))
barplot(table(healthcare$doc), xlab = &quot;binary variable (visits)&quot;,
                         cex.lab = 0.7, cex.axis = 0.7)
table(healthcare$doc)


#health satisfaction across gender
attach(healthcare)
par(mfrow = c(1, 1))
table &lt;- table(gender, hsat)
#reformat the data to be in long format
library(&quot;reshape2&quot;)
table.long &lt;- melt(table, id.vars = &quot;gender&quot;)
table.long
#use ggplot2 to create the plot
#install.packages(&quot;ggplot2&quot;)
library(&quot;ggplot2&quot;)
ggplot(table.long, aes(x=hsat, y=value, fill = factor(gender)))+
  geom_bar(stat=&quot;identity&quot;, position=&quot;dodge&quot;)+xlab(&quot;&quot;)+ylab(&quot;Frequency&quot;)


#use adapted dataset &quot;healthcare1&quot; for regressions
head(healthcare)
healthcare1 &lt;- healthcare
healthcare1$docvis &lt;- NULL
#healthcare1$hsat &lt;- NULL
dev.off()
par(mfrow = c(1, 2))
plot(doc ~ educ, data = healthcare1, ylevels = 2:1, ylab = &quot;&quot;, main = &quot;&quot;)
aux &lt;- glm(doc ~ educ + I(educ^2), data = healthcare1, family = binomial)
edu &lt;- sort(unique(healthcare1$educ))
prop &lt;- ecdf(healthcare1$educ)(edu)
lines(predict(aux, newdata = data.frame(educ = edu),
              type = &quot;response&quot;) ~ prop, col = 2, lwd = 2)

plot(doc ~ age, data = healthcare1, ylevels = 2:1, ylab = &quot;&quot;)
aux &lt;- glm(doc ~ age + I(age^2), data = healthcare1, family = binomial)
ag &lt;- sort(unique(healthcare1$age))
prop &lt;- ecdf(healthcare1$age)(ag)
lines(predict(aux, newdata = data.frame(age = ag),
              type = &quot;response&quot;) ~ prop, col = 2, lwd = 2)

#############
## models
#############
logit &lt;- glm(doc ~ gender + age + I(age^2) + hhinc + hhkids + educ +
                   married + working,
                   family = binomial(link = &quot;logit&quot;),
                   data = healthcare1)
summary(logit)                               
probit &lt;- glm(doc ~ gender + age + I(age^2) + hhinc + hhkids + educ +
                    married + working,
                    family = binomial(link = &quot;probit&quot;),
                    data = healthcare1)
summary(probit)
#Coefficients for probit and logit should differ by a factor of approx. 1.6
cbind(coef(logit), coef(probit)*1.6)
#install.packages(&quot;memisc&quot;)
library(&quot;memisc&quot;)
mtable(logit, probit)
toLatex(mtable(logit, probit))



#############
## predictions and
## interpretations
#############
#use predict function to compute predicted probabilities
predict(logit, data.frame(gender = &quot;male&quot;, age = 44, hhinc = 3500, hhkids = &quot;yes&quot;,
                          educ = 18, married = &quot;yes&quot;, working = &quot;yes&quot;),
                          type = &quot;response&quot;)
predict(logit, data.frame(gender = &quot;female&quot;, age = 44, hhinc = 3500, hhkids = &quot;yes&quot;,
                          educ = 18, married = &quot;yes&quot;, working = &quot;yes&quot;),
                          type = &quot;response&quot;)
predict(probit, data.frame(gender = &quot;male&quot;, age = 44, hhinc = 3500, hhkids = &quot;yes&quot;,
                          educ = 18, married = &quot;yes&quot;, working = &quot;yes&quot;),
                          type = &quot;response&quot;)
predict(probit, data.frame(gender = &quot;female&quot;, age = 44, hhinc = 3500, hhkids = &quot;yes&quot;,
                          educ = 18, married = &quot;yes&quot;, working = &quot;yes&quot;),
                          type = &quot;response&quot;)
#the relation between link-scale and probability scale can be assessed via
predict(logit, data.frame(gender = &quot;male&quot;, age = 44, hhinc = 3500, hhkids = &quot;yes&quot;,
                          educ = 18, married = &quot;yes&quot;, working = &quot;yes&quot;),
                          type = &quot;link&quot;)
mu1 &lt;- exp(-0.1077)/(1+exp(-0.1077))
mu1 #is the same value that we got when setting type = &quot;response&quot;!

#equivalently
predict(logit, data.frame(gender = &quot;female&quot;, age = 44, hhinc = 3500, hhkids = &quot;yes&quot;,
                          educ = 18, married = &quot;yes&quot;, working = &quot;yes&quot;),
                          type = &quot;link&quot;)
mu2 &lt;- exp(0.4568)/(1+exp(0.4568))
mu2

#a male person with the above covariates the probability to visit a
#doctor (yes/no) is 0.8987 times the probability to not visit a doctor.
mu1/(1-mu1) #odds
log(mu1/(1-mu1)) #log Chancen

#a female person with the above covariates the probability to visit a
#doctor (yes/no) is 1.578 times the probability to not visit a doctor.
mu2/(1-mu2) #odds
log(mu2/(1-mu2)) #log Chancen

#odds ratios (ratio of odds for female vs odds for male)
exp(0.564)
exp(0.564) - 1

#similarly:
#education
exp(-0.023) - 1

exp(-0.166) - 1 #hhkids
exp(0.150) -1 #married
exp(-0.073) - 1 #working

#all at once
as.numeric(round(exp(coef(logit)) - 1, 2))


#age is more difficult to interpret due to the quadratic term!
#also hhinc is not reported because it is insignificant and
#virtually zero!



#average predicted probabilities:
mean(predict(logit, type = &quot;response&quot;))
mean(predict(probit, type = &quot;response&quot;))


#predicted probability evaluated at mean regressors
(pred.l &lt;- as.numeric(plogis
         (coef(logit)[1] +
         coef(logit)[2]*(length(gender[gender == &quot;female&quot;])/length(gender) +
         coef(logit)[3]*mean(healthcare1$age) +
         coef(logit)[5]*mean(healthcare1$hhinc) +
         coef(logit)[6]*(length(hhkids[hhkids == &quot;yes&quot;])/length(hhkids)) +
         coef(logit)[7]*mean(healthcare1$educ) +
         coef(logit)[8]*(length(married[married == &quot;yes&quot;])/length(married)) +
         coef(logit)[9]*(length(working[working == &quot;yes&quot;])/length(working))))))
(pred.p &lt;- as.numeric(pnorm
         (coef(probit)[1] +
         coef(probit)[2]*(length(gender[gender == &quot;female&quot;])/length(gender) +
         coef(probit)[3]*mean(healthcare1$age) +
         coef(probit)[5]*mean(healthcare1$hhinc) +
         coef(probit)[6]*(length(hhkids[hhkids == &quot;yes&quot;])/length(hhkids)) +
         coef(probit)[7]*mean(healthcare1$educ) +
         coef(probit)[8]*(length(married[married == &quot;yes&quot;])/length(married)) +
         coef(probit)[9]*(length(working[working == &quot;yes&quot;])/length(working))))))


#calculation for effects for education &quot;by hand&quot;
x.matrix &lt;- model.matrix(logit)
head(x.matrix)

xbar &lt;- colMeans(x.matrix[, 2:9])
xbar
class(xbar)

xbar &lt;- as.data.frame(t(xbar))
xbar

names(xbar)[1] &lt;- &quot;gender&quot;
names(xbar)[5] &lt;- &quot;hhkids&quot;
names(xbar)[7] &lt;- &quot;married&quot;
names(xbar)[8] &lt;- &quot;working&quot;
xbar
class(xbar)

xbar$gender &lt;- replace(xbar$gender, xbar$gender==&quot;0.47877479323721&quot;, &quot;male&quot;)
xbar$hhkids &lt;- replace(xbar$hhkids, xbar$hhkids==&quot;0.402730000731904&quot;, &quot;no&quot;)
xbar$married &lt;- replace(xbar$married, xbar$married==&quot;0.758618165849374&quot;, &quot;yes&quot;)
xbar$working &lt;- replace(xbar$working, xbar$working==&quot;0.677047500548928&quot;, &quot;yes&quot;)
xbar

predict(logit, xbar, type = &quot;response&quot;)
#fixing all covariates at their mean, the &quot;typical&quot; person has a probability to
#visit a doctor of about 55%.

#replicate the data
xbar &lt;- do.call(&quot;rbind&quot;, replicate(111, xbar, simplify = FALSE))
head(xbar)
summary(xbar)
#then instead of having the mean value for &quot;education&quot; in every row we let
#education run from 7 to 18 in steps of 0.1
educ &lt;- seq(7, 18, by = 0.1)
xbar$educ &lt;- educ
xbar
#all other values are fixed on a &quot;typical&quot; value

#then we can say again
predict(logit, xbar, type = &quot;response&quot;)
#and calculated probabilities for a running value of &quot;education&quot;

xbar$prob &lt;- predict(logit, xbar, type = &quot;response&quot;)
xbar
#then we can write
dev.off()
plot(prob ~ educ, data = xbar, type = &quot;l&quot;, ylim = c(0.51,0.61), cex.axis = 0.7,
     cex.lab = 0.7)
#and see that if education runs from 7 to 18, the probability to visit a doctor
#decreases from slightly above 57% to about 51%. In between the function is almost
#linear!


#the whole calculations can now be repeated for &quot;working = no&quot; and the line be
#added to our plot via
x.matrix &lt;- model.matrix(logit)
head(x.matrix)

xbar &lt;- colMeans(x.matrix[, 2:9])
xbar
class(xbar)

xbar &lt;- as.data.frame(t(xbar))
xbar

names(xbar)[1] &lt;- &quot;gender&quot;
names(xbar)[5] &lt;- &quot;hhkids&quot;
names(xbar)[7] &lt;- &quot;married&quot;
names(xbar)[8] &lt;- &quot;working&quot;
xbar
class(xbar)

xbar$gender &lt;- replace(xbar$gender, xbar$gender==&quot;0.47877479323721&quot;, &quot;male&quot;)
xbar$hhkids &lt;- replace(xbar$hhkids, xbar$hhkids==&quot;0.402730000731904&quot;, &quot;no&quot;)
xbar$married &lt;- replace(xbar$married, xbar$married==&quot;0.758618165849374&quot;, &quot;yes&quot;)
xbar$working &lt;- replace(xbar$working, xbar$working==&quot;0.677047500548928&quot;, &quot;no&quot;)
xbar

predict(logit, xbar, type = &quot;response&quot;)
xbar &lt;- do.call(&quot;rbind&quot;, replicate(111, xbar, simplify = FALSE))
head(xbar)
summary(xbar)
educ &lt;- seq(7, 18, by = 0.1)
xbar$educ &lt;- educ
xbar
predict(logit, xbar, type = &quot;response&quot;)
xbar$prob &lt;- predict(logit, xbar, type = &quot;response&quot;)
xbar
lines(prob ~ educ, data = xbar, type = &quot;l&quot;, col = &quot;red&quot;)
legend(&quot;bottomleft&quot;, c(&quot;working = yes&quot;, &quot;working = no&quot;),
       lty = c(1, 1), lwd = 2, col = c(&quot;black&quot;, &quot;red&quot;), bty = &quot;n&quot;, cex=0.7)
#and see that if education runs from 7 to 18, the probability to visit a doctor
#decreases from slightly above 57% to about 51%. In between the function is almost
#linear!


#effects plots with the effects-package
#install.packages(&quot;effects&quot;)
library(&quot;effects&quot;)
#to use the effects plot we restrict ourselves to the logit-model and reestimate
#it so that R understands that &quot;age&quot; and &quot;age^2&quot; belong to one and the same
#variable. this is done via &quot;poly()&quot;
logit.2 &lt;- glm(doc ~ gender + poly(age, 2, raw = TRUE) + hhinc + hhkids +
                   educ + married + working,
                   family = binomial(link = &quot;logit&quot;),
                   data = healthcare1)
logit.eff &lt;- allEffects(logit.2)
plot(logit.eff, aks = FALSE, rescale.axis = FALSE, cex.axis = 0.7, cex.lab = 0.7,
     main = &quot;&quot;, rug = FALSE)


#############
## goodness of fit
#############
#McFadden R^2
#Calculations:
#evaluating the constant-only model:
logit0 &lt;- glm(doc ~ 1, data = healthcare1, family = binomial(link = &quot;logit&quot;))
logLik(logit0)
probit0 &lt;- glm(doc ~ 1, data = healthcare1, family = binomial(link = &quot;probit&quot;))
logLik(probit0)

#the log-likelihood values from the unrestricted (full) model from above:
logLik(logit)
logLik(probit)


rsq.logit &lt;- as.numeric(1-(logLik(logit)/logLik(logit0)))
rsq.logit
rsq.probit &lt;- as.numeric(1-(logLik(probit)/logLik(probit0)))
rsq.probit


#Alternative Approach
## &quot;Proportion of Correct Predictions&quot;
#with cut-off value 50%
conftab1 &lt;- table(true = healthcare1$doc, pred = fitted(logit) &gt; 0.5) #Kreuztabelle
#confusion-Matrix mit logit
conftab2 &lt;- table(true = healthcare1$doc, pred = fitted(probit) &gt; 0.5) #Kreuztabelle
#confusion-Matrix mit probit
mtable(conftab1, conftab2)
toLatex(mtable(logit, probit))


#install.packages(&quot;ROCR&quot;)
library(ROCR)
pred &lt;- prediction(fitted(logit), healthcare1$doc)
plot(performance(pred, &quot;acc&quot;))
plot(performance(pred, &quot;tpr&quot;, &quot;fpr&quot;))
abline(0, 1, lty = 2)


##############
## LR-Test
##############
#Likelihood Ratio test: raw calculations
as.numeric(-2*(logLik(probit0)-logLik(probit))) #result: 1319 (relevant test
#statistic)
as.numeric(-2*(logLik(logit0)-logLik(logit))) #result: 1326
qchisq(0.99, 8) #we have 8 degress of freedom
#-&gt; Hypothesis that the two models are the same can be rejected!


#LR-test with package:
#install.packages(&quot;lmtest&quot;)
library(lmtest)
lrtest(probit0, probit)
lrtest(logit0, logit)</code></pre>
</div>
<div id="footnotes" class="section level4">
<h4>Footnotes</h4>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>This can be derived from
<span class="math inline">\(E[\mathbf{y}\vert \mathbf{X}]=E[\mathbf{X\beta+\epsilon}\vert \mathbf{X}]=E[\mathbf{X\beta\vert X}]+E[\mathbf{\epsilon\vert X}]=\mathbf{X\beta}\)</span>.<a href="#fnref1" class="footnote-back">↩</a></p></li>
<li id="fn2"><p>Note that normality of the errors is not necessary for the tests
to hold and also not part of the Gauß-Markov-assumptions.<a href="#fnref2" class="footnote-back">↩</a></p></li>
<li id="fn3"><p>Generalized linear models were formulated by <span class="citation">@nelder:72</span> as a way
of unifying various other statistical models, including linear
regression, logistic regression and Poisson regression.<a href="#fnref3" class="footnote-back">↩</a></p></li>
<li id="fn4"><p>If the dispersion parameter <span class="math inline">\(\phi\)</span> is fixed then this type of
distributions are called an <em>exponential family</em>.<a href="#fnref4" class="footnote-back">↩</a></p></li>
<li id="fn5"><p>Due to <span class="math inline">\(g(\mu_i)=\eta_i=\mathbf{x_i&#39;\beta}\)</span> we have
<span class="math inline">\(\mu_i=g^{-1}(\eta_i)=g^{-1}(\mathbf{x_i&#39;\beta})\)</span> where <span class="math inline">\(g^{-1}\)</span> is
the inverse link function.<a href="#fnref5" class="footnote-back">↩</a></p></li>
<li id="fn6"><p>Here we denote the conditional expectation <span class="math inline">\(\mu\)</span> as our success probability <span class="math inline">\(p\)</span> which is <span class="math inline">\(\in[0, 1]\)</span>.<a href="#fnref6" class="footnote-back">↩</a></p></li>
<li id="fn7"><p>It is important to note that <span class="math inline">\(b(.)\)</span> is a function of <span class="math inline">\(\theta\)</span> in
the first place so that we have to differentiate with respect to
<span class="math inline">\(\theta\)</span> and not <span class="math inline">\(\mu\)</span>.<a href="#fnref7" class="footnote-back">↩</a></p></li>
<li id="fn8"><p>In the case of the normal distribution mean and variance are
independent.<a href="#fnref8" class="footnote-back">↩</a></p></li>
<li id="fn9"><p>Details will follow below.<a href="#fnref9" class="footnote-back">↩</a></p></li>
<li id="fn10"><p>In the setting of GLMs the dispersion parameter <span class="math inline">\(\phi\)</span> can be
treated as a nuisance parameter or is known anyway.<a href="#fnref10" class="footnote-back">↩</a></p></li>
<li id="fn11"><p>Whereby <span class="math inline">\(F(.)\)</span> can then be either the CDF of the normal or the logistic distribution.<a href="#fnref11" class="footnote-back">↩</a></p></li>
<li id="fn12"><p>Switching from &quot;likelihood&quot; to &quot;densities&quot; emphasizes the
reversed roles of the data and the parameters (i. e., the arguments)
and is also expressed notationally by switching from the joint
distribution as the joint probability density for observing
<span class="math inline">\(y_1, ..., y_n\)</span> given <span class="math inline">\(\theta\)</span> to the <em>likelihood function</em> of the
unknown parameter <span class="math inline">\(theta\)</span> given a random sample <span class="math inline">\(y_1, ..., y_n\)</span>.<a href="#fnref12" class="footnote-back">↩</a></p></li>
<li id="fn13"><p>Here write down the setup without regressors; the general considerations are the same for cases where we specifically consider regressors, i.e. for <span class="math inline">\(f(\theta; y_1, ..., y_n\vert\mathbf{x})\)</span>.<a href="#fnref13" class="footnote-back">↩</a></p></li>
<li id="fn14"><p>A detailed list with the variables and their description can be
found in the accompanying R code in the
Appendix below.<a href="#fnref14" class="footnote-back">↩</a></p></li>
<li id="fn15"><p>The logit model can therefore be interpreted as a
semi-logarithmic model for the odds.<a href="#fnref15" class="footnote-back">↩</a></p></li>
<li id="fn16"><p>Evaluating the predicted probabilities using the probit model
instead results in virtually no difference.<a href="#fnref16" class="footnote-back">↩</a></p></li>
<li id="fn17"><p>The default would be <code>type = &quot;link&quot;</code> that reports the
corresponding value on the link-scale.<a href="#fnref17" class="footnote-back">↩</a></p></li>
<li id="fn18"><p>Note that these odds ratios don’t change, i.e., the remain
constant even if we change the other regressors, whereas the
difference on the probability scale is not the same.<a href="#fnref18" class="footnote-back">↩</a></p></li>
<li id="fn19"><p>Note that the calculations were made only using the logit-model.
Applying the procedure to probit-models would result in virtually
identical results.<a href="#fnref19" class="footnote-back">↩</a></p></li>
<li id="fn20"><p>The pseudo-<span class="math inline">\(R^2\)</span> uses the same information as the likelihood
ratio test.<a href="#fnref20" class="footnote-back">↩</a></p></li>
<li id="fn21"><p>It is also possible to check for which cutoff-value the
proportion of correct predictions is largest (see the R-Code in the Appendix below).<a href="#fnref21" class="footnote-back">↩</a></p></li>
</ol>
</div>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5 0 11V3C0 1.5.8.8.8.8S1.5 0 3 0h8c1.5 0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 1 0 0-6 3 3 0 0 0 0 6z"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/r/" rel="tag">R</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/econometrics/" rel="tag">Econometrics</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/logistic-regression/" rel="tag">Logistic Regression</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>


<nav class="pager flex">
	<div class="pager__item pager__item--next">
		<a class="pager__link" href="/post/2015-01-21-numeric-integration/" rel="next">
			<span class="pager__subtitle">Next&thinsp;»</span>
			<p class="pager__title">Simple Numerical Integration Techniques with Python</p>
		</a>
	</div>
</nav>



			</div>
			<aside class="sidebar"><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCH..." value="" name="q" aria-label="SEARCH...">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="sitesearch" value="/" />
	</form>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/post/2018-03-13-apis-and-web-services-background-and-an-example/">APIs and Web Services: Background and an Example</a></li>
			<li class="widget__item"><a class="widget__link" href="/post/2018-03-09-collapsible-tree-with-d3js/">A Collapsible Tree with D3.js</a></li>
			<li class="widget__item"><a class="widget__link" href="/post/2018-03-09-a-markdown-crash-course/">A Markdown Crash-Course</a></li>
			<li class="widget__item"><a class="widget__link" href="/post/2018-02-18-candlestick-chart-with-bokeh/">Candlestick Chart using Python&#39;s Bokeh Library</a></li>
			<li class="widget__item"><a class="widget__link" href="/post/2017-09-27-contribution-ecb-group-of-experts-of-business-registers/">Expert Group on Business Registers, 27-29 September 2017, OECD, Paris</a></li>
		</ul>
	</div>
</div>
<div class="widget-categories widget">
	<h4 class="widget__title">Categories</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item">
				<a class="widget__link" href="/categories/econometrics/">Econometrics</a>
			</li>
			<li class="widget__item">
				<a class="widget__link" href="/categories/javascript/">javascript</a>
			</li>
			<li class="widget__item">
				<a class="widget__link" href="/categories/markdown/">Markdown</a>
			</li>
			<li class="widget__item">
				<a class="widget__link" href="/categories/publication/">Publication</a>
			</li>
			<li class="widget__item">
				<a class="widget__link" href="/categories/python/">Python</a>
			</li>
		</ul>
	</div>
</div>
<div class="widget-taglist widget">
	<h4 class="widget__title">Tags</h4>
	<div class="widget__content">
		<a class="widget-taglist__link widget__link btn" href="/tags/api/" title="API">API</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/d3.js/" title="D3.js">D3.js</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/econometrics/" title="Econometrics">Econometrics</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/flask/" title="Flask">Flask</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/gleif/" title="GLEIF">GLEIF</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/javascript/" title="javascript">javascript</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/logistic-regression/" title="Logistic Regression">Logistic Regression</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/markdown/" title="Markdown">Markdown</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/numerical-integration/" title="Numerical Integration">Numerical Integration</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/publication/" title="Publication">Publication</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/python/" title="Python">Python</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/r/" title="R">R</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/web-scraping/" title="Web-Scraping">Web-Scraping</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/web-service/" title="Web Service">Web Service</a>
	</div>
</div>
<div class="widget-social widget">
	<h4 class="widget-social__title widget__title">Social</h4>
	<div class="widget-social__content widget__content">
		<div class="widget-social__item widget__item">
			<a class="widget-social__link widget__link btn" title="Twitter" rel="noopener noreferrer" href="https://twitter.com/marcel_kay_mav" target="_blank">
				<svg class="widget-social__link-icon icon icon-twitter" width="24" height="24" viewBox="0 0 384 312"><path d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5 0-78.8 35.3-78.8 78.8 0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3-6.7 11.6-10.6 25.2-10.6 39.6 0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1 0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4 0-12.6-.4-18.8-1.1 34.9 22.4 76.3 35.4 120.8 35.4 144.9 0 224.1-120 224.1-224.1 0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg>
				<span>Twitter</span>
			</a>
		</div>
		<div class="widget-social__item widget__item">
			<a class="widget-social__link widget__link btn" title="LinkedIn" rel="noopener noreferrer" href="https://linkedin.com/in/marcel-kropp-028a17154" target="_blank">
				<svg class="widget-social__link-icon icon icon-linkedin" width="24" height="24" viewBox="0 0 352 352"><path d="M0,40v272c0,21.9,18.1,40,40,40h272c21.9,0,40-18.1,40-40V40c0-21.9-18.1-40-40-40H40C18.1,0,0,18.1,0,40z M312,32 c4.6,0,8,3.4,8,8v272c0,4.6-3.4,8-8,8H40c-4.6,0-8-3.4-8-8V40c0-4.6,3.4-8,8-8H312z M59.5,87c0,15.2,12.3,27.5,27.5,27.5 c15.2,0,27.5-12.3,27.5-27.5c0-15.2-12.3-27.5-27.5-27.5C71.8,59.5,59.5,71.8,59.5,87z M187,157h-1v-21h-45v152h47v-75 c0-19.8,3.9-39,28.5-39c24.2,0,24.5,22.4,24.5,40v74h47v-83.5c0-40.9-8.7-72-56.5-72C208.5,132.5,193.3,145.1,187,157z M64,288h47.5 V136H64V288z"/></svg>
				<span>LinkedIn</span>
			</a>
		</div>
		<div class="widget-social__item widget__item">
			<a class="widget-social__link widget__link btn" title="GitHub" rel="noopener noreferrer" href="https://github.com/himynameismarcel" target="_blank">
				<svg class="widget-social__link-icon icon icon-github" width="24" height="24" viewBox="0 0 384 374"><path d="m192 0c-106.1 0-192 85.8-192 191.7 0 84.7 55 156.6 131.3 181.9 9.6 1.8 13.1-4.2 13.1-9.2 0-4.6-.2-16.6-.3-32.6-53.4 11.6-64.7-25.7-64.7-25.7-8.7-22.1-21.3-28-21.3-28-17.4-11.9 1.3-11.6 1.3-11.6 19.3 1.4 29.4 19.8 29.4 19.8 17.1 29.3 44.9 20.8 55.9 15.9 1.7-12.4 6.7-20.8 12.2-25.6-42.6-4.8-87.5-21.3-87.5-94.8 0-20.9 7.5-38 19.8-51.4-2-4.9-8.6-24.3 1.9-50.7 0 0 16.1-5.2 52.8 19.7 15.3-4.2 31.7-6.4 48.1-6.5 16.3.1 32.7 2.2 48.1 6.5 36.7-24.8 52.8-19.7 52.8-19.7 10.5 26.4 3.9 45.9 1.9 50.7 12.3 13.4 19.7 30.5 19.7 51.4 0 73.7-44.9 89.9-87.7 94.6 6.9 5.9 13 17.6 13 35.5 0 25.6-.2 46.3-.2 52.6 0 5.1 3.5 11.1 13.2 9.2 76.2-25.5 131.2-97.3 131.2-182 0-105.9-86-191.7-192-191.7z"/></svg>
				<span>GitHub</span>
			</a>
		</div>

		
	</div>
</div>
</aside>
		</div>
		<footer class="footer">
	<script src="//yihui.org/js/math-code.js"></script>
<script async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2020 himynameismarcel.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>

	</div>
<script async defer src="/js/menu.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>